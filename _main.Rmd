--- 
title: "STAT 107 Outline of Class Notes"
author: "Rebecca Kurtz-Garcia"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: https://rpkgarcia.github.io/stat107_summer2022notes/
#cover-image: images/logo_stats.png
description: |
  Course notes for UCR's STAT 107.  An introduction to programming in R.
link-citations: yes
github-repo: rstudio/bookdown-demo
mainfont: Arial
monofont: "Courier New"
runtime: shiny
#monotonoptions: "Scale =0.7"
---

# Welcome {-}

Placeholder



<!--chapter:end:index.Rmd-->


# Introduction to R

Placeholder


## Download and Install R and Rstudio
## The **RStudio** Interface 
### Bottom Left Pane {-}
### Bottom Right Pane {-}
### Top Right Pane {-}
### Top Left Pane {-}
## Comments 
## Operators {#operators}
### Arithmetic Operators {-}
### Relational Operators {-}
### Logical operators {-}
### Assignment Operators {-}
## Naming Conventions
## Basic Calculations
## Floating Point Error 
## Additional Resources {-}

<!--chapter:end:01-intro.Rmd-->


# Atomic Vectors  

Placeholder


## Examples 
### Double {-}
### Integer {-}
### Logical {-}
### Character {-}
## Basic Features 
### Length
### Vectorized Operations and Recylcing {#vecoperators}
### Coercion 
### Testing
### Names {#names}
### typeof() and class()
### Accessing Elements of a Vector {#index}
## Summary {-}
## Additional Resources {-}

<!--chapter:end:02-atomic-vectors.Rmd-->


# Factors and Lists

Placeholder


## Factors 
### Levels
### Ordered Factor
## Factors - Basic Features 
### Length 
### Coercion 
### Testing/Class
### Names 
### Accessing Elements
### Frequency Tables 
## Lists 
### Nested Lists 
## List - Basic Features 
### Length 
### Coercion 
### Testing/Class
### Names
### Accessing Elements
## Summary {-}
## Additional Resources {-}

<!--chapter:end:03-factors-and-lists.Rmd-->


# Matrices, Data Frames, and More 

Placeholder


## Matrices
### Vectorized Operations 
## Data Frames 
### Column Names
## Basic Features of Matrices/Data-Frames
### Dimensions 
### Accesing Elements 
### Coercion 
### Testing/Class
### Names 
## Other Object Types and the Global Environment
## Summary {-}

<!--chapter:end:04-matrices-and-dataframes.Rmd-->


# Indexing

Placeholder


## Atomic Vectors and Factors {#VectorIndex}
### Example: Positive Integers {-}
### Example: Negative Integers {-}
### Example: Logical Values {-}
### Example: Names {-}
## Lists 
### Double Brackets
### Dollar Sign 
## Matrices
### Two Vectors 
### Single Vector
## Data frames 
### Example: Double brackets {-}
### Example: Dollar Sign {-}
## Features and Applications 
### Indexing and Reassignment 
### Ordering/Integer Indexing
### Adding Elements/Rows/Columns
### Delete Elements/Rows/Columns
### Select Based on Condition
### Convert Indexing Techniques
## Summary {-}

<!--chapter:end:05-indexing.Rmd-->


# Working with Data Sets

Placeholder


## Getting Data Sets in Our Working Environment {#LoadData}
### Built-In Data {-}
### Importing From Your Computer{-}
### Import From Online {-}
## Basic Data Manipulation
### Subset {-}
### Adding Columns {-}
### NA values {-}
### NULL {-}
### Adding Rows {-}
## Summary {-}

<!--chapter:end:06-loading-data.Rmd-->


# Functions {#functions}

Placeholder


## Build Your Own Function
## Lexical Scoping 
## Built-In Functions
## Vectorization 
## Help Files 
## The ... Argument {#dots}
## Generic Functions 
## Additional Resources {-}

<!--chapter:end:07-functions.Rmd-->


# If Statements

Placeholder


## If 
## If Else 
## Else If 
## Nested If Chains
## Ifelse {#ifelse}
## Vectorizing If-Statments {#vectorizeifelse}
## Additional Resources {-}

<!--chapter:end:08-if-statements.Rmd-->


# Packages {#packages}

Placeholder


## Namespace Collisions  

<!--chapter:end:09-packages.Rmd-->


# Text Data

Placeholder


## Making Strings
## Substring Operations
## Dividing Strings into Vectors
## Converting Objects into Strings
## Versatility of the paste() Function
## Substitution 
## Text of Some Importance
## Search
## Vectorizing Text Functions
## Regular Expressions
## Word Count Tables 
## Wordcloud Package

<!--chapter:end:10-text-analysis.Rmd-->


# Regular Expressions {#Regex}

Placeholder


## Ranges, Escaping
## Metacharacters 
## Quantifiers 
## Anchoring 
## Splitting on a Regexp 
## Stringr Package
## Wordcloud Package
## Replacements
## Additional Resources {-}

<!--chapter:end:11-regular-expressions.Rmd-->


# Base R Plotting

Placeholder


## Load A Big Data Set
## Histograms 
## Boxplot
## Scatter Plot
## Pie Charts
## Adding Straight Lines 
## Adding Lines Between Points 
## Adding Individual Points 
## The par() Help File 
## Adding Legends
## What makes a good plot? 

<!--chapter:end:12-baseR-plotting.Rmd-->


# Extending Base R Graphics

Placeholder


## The "maps" Package
## Some additional Packages to consider

<!--chapter:end:13-Extending-Base-R-Graphics.Rmd-->


# Loops {#loops}

Placeholder


## While Loop
## For Loops
## Break 
## Next 
## Nested Loops
## Examples 
### Example 1
### Example 2
## Additional Resources

<!--chapter:end:14-loops.Rmd-->

# Simulation 


## What Are Simulation Studies

Simulation studies are used in statistics to verify theoretical results, and to do "what-if" experiments.  The are commonly used in a variety of applications and fields.  Often times we have a particular theory or an idea about how things work.  In order to verify that our theory is correct we can simulate a data set with known properties, and then check if the data set matches our theory. For example, suppose you want to calculate a 95\% confidence interval for the heights of all college students. Our theory on confidence intervals says that we expect that a 95\% confidence interval will capture the true mean about 95\% of the time. If we simply go out and take a sample of data from "real-life" and then calculate the a confidence interval we have no idea if that confidence interval captures the truth because the true average height of all college students is unknown!  It is not feasible to sample every college students, so we will never know if the true mean actually falls within our interval. Furthermore, in this example we only calculated one confidence interval.  If we did know the true mean then our results would simply indicate that the confidence interval captured the mean or not, but nothing about the *rate* that confidence intervals capture the mean, which should be 95\%. What we can do instead is simulate this process.  We can randomly generate data that has a true mean of $\mu$ using a computer.  Then we can estimate a 95\% confidence interval using the appropriate methods and see if this confidence interval contains $\mu$.  We then can repeat this process as many times as we would like because we are using a computer, which makes the process much simpler.  If our theory about confidence intervals is correct we would expect that 95\% of our confidence intervals captured the true mean ($\mu$), and the rest did not. 

Simulations are very important, and can range greatly in complexity.  We will focus on simple techniques in this section. For a more complete discussion on simulation studies in statistics, and best practices see this freely available online at [https://onlinelibrary.wiley.com/doi/10.1002/sim.8086](https://onlinelibrary.wiley.com/doi/10.1002/sim.8086).  

## Review Random Variables 

Before discussing how to simulate data with specific properties we review the concept of a random variable.  This discussion is adapted primarily from Chapter 3 and 4 of @OpenIntro.  A **random variable** is a random process or variable with a numeric outcome. Random variables in general have a typical value which is called the **expected value**.  However, in practice random variables also have variability, that is we do not expect to obtain the expected value for every random variable.  Instead we expect to observe a range of values according to some sort of pattern, which is usually centered around the expected value. 

For example, in Yellowstone National Park, Wyoming there is a famous geyser called Old Faithful.  A geyser is a natural hot spring that periodically erupts water.  The amount of time (in minutes) the eruption lasts varies each time, but not by a dramatic amount.  The eruption time in minutes is the random variable, and the expected value is the average eruption length, and we can measure the variability using a measure of dispersion, like the variance.  Below is a histogram of a random sample of eruptions times of 272 different eruptions. 

```{r}
hist(faithful$eruptions, 
     main = "Eruption Times of Old Faithful", 
     xlab = "Time (in minutes)", 
     breaks = seq(0, 7, by = .5))
```


As mentioned previously, random variables generate observations according to some sort of pattern.  These patterns are referred to as **densities**, and can be analyzed visually, and analytically (i.e. through formulas).  One of the most common ways to look at how often we would expect certain values for data that we find from the "real-world" is to use a histogram.  Histogram order observations into mutually exclusive bins, where the height of each bin indicates how common a particular range of values is.  For example, in the histogram above each bin is a 30 second (0.5 minute) range, and we can see the frequency of observations within the data set that are in this range on the y-axis.  When thinking about random variables though, it is often more helpful to look at the proportion of observed values within a bin, instead of the frequency.  We can modify our histogram to accomplish this. 


```{r}
hist(faithful$eruptions, 
     main = "Eruption Times of Old Faithful", 
     xlab = "Time (in minutes)", 
     breaks = seq(0, 7, by = .5), 
     freq = FALSE)
```


Notice that the y-axis now says "Density".  This is the relative probability that an observed data point will be a particular value.  In "real-world" data, we can only estimate the density.  We will never know the truth.  Hence, we will never know the true expected value, or the true variance.  This is why we have statistics!!  We have statistics in order to make educated guesses about these values and properties from which the data arises from.  


Analyzing the density or pattern of a random variable visually with histograms is intuitive, but limiting.  Using a formula as a representation for a density is much more versatile and useful. For these functions we have **parameters** which are values that let us further customize the function representation of a density to the specific random variable that we have. Let $X$ be a random variable, and $x$ be some observed value. In practice, if $X$ is a random variable that can only take on values that are discrete (usually integers) then we say the probability of observing some a particular value, say $x$, is denoted $p(x)$. There are extremely well known discrete densities that random variables tend to follow and that are available in R.   A few of them are listed below. 

- **Binomial**: 

$$p(x) = \binom{n}{x} p^x (1-p)^{n-x} \hspace{1cm} x = 0, ..., n$$

- **Poison**: 
$$p(x) = λ^x exp(-λ)/x! \hspace{1cm} x = 0, ..., n$$

- **Negative Binomial:**

$$Γ(x+n)/(Γ(n) x!) p^n (1-p)^x   \hspace{1cm} x = 0, ..., $$

If $X$ is a random variable that can be any value within a range of numbers then we denote the density function of this random variable by $f(x)$. As with discrete random variables, there are extremely well known continuous densities that random variables tend to follow and that are available in R.   A few of them are listed below. 


- **Uniform:**

$$f(x) = 1/(max-min)$$


- **Log-Normal:**

$$f(x) = 1/(√(2 π) σ x) e^{-((log{(x)} - μ)^2 / (2 σ^2))} \hspace{1cm} 0 <x < \infty$$

- **Exponential:**

$$f(x) = λ {e}^{- λ x} \hspace{1cm} 0 <x < \infty$$

- **Normal**:
$$f(x) = 1/(√(2 π) σ) e^{-((x - μ)^2/(2 σ^2))} \hspace{1cm} -\infty <x < \infty$$

- **t-Distribution**

$$f(x) = Γ((n+1)/2) / (√(n π) Γ(n/2)) (1 + x^2/n)^-((n+1)/2)\hspace{1cm} -\infty <x < \infty$$

To see a complete list of well known distribution functions (or densities) that R already has see the help file `?Distributions`. 

## Generating Random Variables

Each distribution available in base R is listed in `?Distributions`.  Each of the distributions has a link to their corresponding help file which lists four main functions: `dxxx()`, `pxxx()`, `qxxx()` and `rxxx()`. These are the density function, cumulative distribution function, quantile function, and a random number generator for the particular random variable of interest.  The letters `xxx` are replaced by an code for the particular random variable. The density function (`dxxx()`) generates the relative probability of observing a particular value. The cumulative distribution function (`pxxx()`) generates the probability of observing a particular value, and anything smaller than this value.  The quantile function (`qxxx()`) generates what value corresponds to a given percentile.  Lastly, the random number generator (`rxxx()`) will generate a random variable according to the given distribuiton/density. We will focus only on `dxxx()` and `rxxx()`. 

For example, go to `?Distributions` and click on `dnorm` which corresponds to the normal distribution (in the bottom third of the list).  Here you will see the functions `dnorm()`, `pnorm()`, `qnorm()`, and `rnorm()`.   To generate a normal random variable with mean 0 and standard deviation 1 we can use the following command. 


```{r}
# Generate a single normal random variable with mean 0 and standard deviation 1
rnorm(1)


# Generate 20 normal random variables with mean 0 and standard deviation 1
rnorm(20)
```


However, as we saw in the previous section, these common distribution functions have parameters which let us further customize the behavior of the random variable. We can change the parameters of any of the distributions in R using the function arguments. 

```{r}
# Normal random variable with mean 10, and standard deviation 0.5 
rnorm(1, mean = 10, sd = 0.5)

# Normal random variable with mean 10, and standard deviation 0.5 
rnorm(20, mean = 10, sd = 0.5)
```


To find the relative probability of observing a particular value for a normal distribution we use `dnorm()`. 


```{r}
# Relative probability of observing the value 0.25
# for a normal random variable with mean 0 and standard devation 1
dnorm(0.25)

# Relative probability of observing the value 0.25, -0.5, 2
# for a normal random variable with mean 0 and standard devation 1
dnorm(c(0.25, -0.5, 2))
```


## Setting a Seed

Often times when running a simulation we will want to use the same numbers over and over again.  The function `rxxx()` generates values randomly though, which means each time we call this function we will get new and different values.  In order to make sure our values are consistent each time we run R, or across computers we can set the seed.  The seed determines the way the computer generates our random numbers. Normally the seed is random, so each time `rxxx()` is called we get a new random sequence.  Using the `set.seed()` function will make sure that the sequence stays the same each time we run `rxxx()`. 

See the example below were generate a sequence of 10 uniform random variables that are between -5 and 5. If you copy this code directly into your computer you will get the same sequence.  However, if you do not set the seed you will get a different sequence. 


```{r}
set.seed(1)
runif(10, min = -5, max = 5)
```


## sample()

The `sample()` function is a powerful tool that can let you create your own unique random sequence.  You can sample values with replacement, or without replacement.  You can also assign certain probabilities to certain events.  This is a particularly helpful function for rearranging rows, and for generating a sequence of categorical variables. For example, we can simulate a coin flip. 


```{r}
sample(c("Heads", "Tails"), size=1 , prob = c(0.5, 0.5))
```

Below is the code for simulating 10 coin flips. 

```{r}
sample(c("Heads", "Tails"), size=10 , prob = c(0.5, 0.5), replace = TRUE)
```

We can set the seed like we did before in order to insure we keep getting the same sample each time we run this code.  

```{r}
set.seed(1)
sample(c("Heads", "Tails"), size=10 , prob = c(0.5, 0.5), replace = TRUE)
```


## Cumulative-Proportion/Cumulative-Win Plots

A fair question to ask with simulations is: "how many simulations do we need for our data to representative?". There are rigorous rules that we can use to determine how many simulations we need until our data set starts to represent the truth; however, we will focus on summary tables and visualization tools for now.  We know we have enough simulations when our data set seems to "stabilize". We track statistics or estimated probability of an event changes as we consider more data. We can visualize this using a cumulative proportion plot.  For example, suppose we want to estimate how often we can expect a coin to land on a "head". We can flip a coin multiple times and record a "1" if it land on a head, and "0" otherwise.  Then we can calculate the proportion of the times that the coin landed on a head, which is the (arithmetic) mean of the 0s and 1s.  If we only flip the coin a few times, then the proportion that we calculate may not be accurate due to random fluctuation.  However, the more coin flips we consider the more likely the proportion we calculate approaches the truth.  We can observe this using a cumulative proportion plot.  In a cumulative proportion plot we have the number of trials on the x-axis, and the estimated proportion of successes using all data up to trial "x" on the y-axis. For example, suppose we flip one coin five times and obtain the results 1, 1, 0, 1, 0.  Then the proportion of successes using "x" number of trials is the following.

- When the number of trials is x = 1, the proportion of times we observe a head is y = (1)/1 = 1
- When the number of trials is x = 2, the proportion of times we observe a head is y = (1+1)/2 = 1
- When the number of trials is x = 3, the proportion of times we observe a head is y = (1+1+0)/3 = 2/3
- When the number of trials is x = 4, the proportion of times we observe a head is y = (1+1+0+1)/4 = 3/4
- When the number of trials is x = 5, the proportion of times we observe a head is y = (1+1+0+1+0)/5 = 3/5


```{r}
library(shiny)
shinyAppFile(
  "shiny/one_coin/app.R",
  options = list(width = "100%", height = 700)
)
```


## replicate() 

The `replicate()` function repeats a function call `n` times in a very efficient way.  Suppose we wish to find the probability we get exactly 3 heads if we flip a coin 10 times.  We can replicate flipping a coin 10 times, and count how many of these times we see exactly 3 heads. 


```{r}
coin_flip_heads3 <- function(){
        coin_flip <- sample(c("Heads", "Tails"), size=10 , 
                           prob = c(0.5, 0.5), replace = TRUE)
        num_heads <- length(which(coin_flip == "Heads"))
        
        if(num_heads == 3){
                heads3 = TRUE
        } else{
                heads3 = FALSE
        }
        return(heads3)
}



# Generates flipping a coin 3 times
# Returns TRUE if exactly 3 flips resulted in heads
# Returns FALSE if otherwise 
coin_flip_heads3()



# Replicate the experiment 10,000 times
see_heads3 <- replicate(10000, coin_flip_heads3())

# How many of these experiments results in 3 heads???
# Probability of seeing exactly 3 heads is approximately 12%
table(see_heads3)/10000
```


## Adding curves to graphs

Often times when generating data or using a real-world data set we might want to see how well a well known distribution fits a particular data set.  We can do so by adding the proposed density function on to a histogram which contains the data set of interest.  For example, suppose we wish to see if a normal distribution with mean of 3.5 and standard deviation of 1.14 fits the old faithful data set for eruption times. We can add this density function on top and see if it approximately fits the data. 


```{r}
hist(faithful$eruptions, 
     main = "Eruption Times of Old Faithful", 
     xlab = "Time (in minutes)", 
     breaks = seq(0, 7, by = .5), 
     freq = FALSE)

curve(dnorm(x, 3.5, 1.14), add = TRUE)
```

Now lets generate 272 normal random variables with mean 3.5 and standard deviation of 1.14 and see how well are generated data matches the density function. 

```{r}
gen_data <- rnorm(272, mean = 3.5, sd = 1.14)

hist(gen_data, 
     main = "Simulated Data", 
     xlab = "X", 
     breaks = seq(-0.5, 7, by = .5), 
     freq = FALSE)

curve(dnorm(x, 3.5, 1.14), add = TRUE)
```

We do not see a perfect fit, but this is what happens with random samples.  The fit for the simulated data is what we expect approximately if we had a sample with this distribution. It should be approximately the same as the curve, which we see for the simulated data. Our old faithful data is significantly less fitted the simulated data.  Thus it appears that the old faithful eruption times is probably not normally distributed with mean 3.5 and standard deviation 1.14.  How similar is "similar enough" is not a hard and fast rule, and it is ultimately up to the researcher.  The simulated data set gives us a general idea of what is "similar enough".


Note that the `curve()` function does not need to be added to graph but can stand alone.  

```{r}
curve(dnorm(x, 3.5, 1.14), 0, 7, add = FALSE)
```

In addition, we can make our own functions to plot using `curve()`. 

```{r}
my_fun <- function(x){
        y = x^2 + 2*x - 3
        return(y)
}

curve(my_fun(x), -10, 10)
```


## Example: Central Limit Theorem (CLT)

### Recall CLT 

Let $X_1, ..., X_n$ be independent and identically distributed random variables with mean and variance, $\mu$ and $\sigma^2$. If the sample size is sufficiently large ($n \geq 30$), the sample mean $\bar{x}$ will tend to follow a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$.

In other words, for any set of data $X_1, ..., X_n$ that is independent and comes from the same distribution, and that distribution has a finite mean and variance, $\mu$ and $\sigma^2$.  Then 

$$\bar{x} \sim N \left ( \mu, \frac{\sigma}{\sqrt{n}} \right ) $$

The distribution above is a *sampling distribution*.  It is the distribution of a *sample mean*.  That is, if we take $k$ samples, and for each sample we calculated the mean, the central limit theorem tells us about the distribution of these means ($\bar{x}_1, \dots, \bar{x}_k$). 

###  Assumption Violations of CLT 

The central limit theorem has a few key assumptions.  Some of these assumptions are fairly easy to meet, and others are more susceptible to being violated. For example, typically it is reasonable to assume that mean and variance are finite. Other assumptions are easier to violate, and the consequences of violating these assumptions vary greatly.  Sometimes we may have data that comes from a mix of multiple distributions, this violate the assumption that data is identically distributed.  Another assumption violation could be independence.  For example, in time series data we typically observe a random variable over a sequence of time and measure it repeatedly. This would violate the independence assumption because earlier observations are typically related to future observations. Another assumption that is often violated is the minimum sample size. The central limit theorem says we need at least 30 observations, or else we should use the t-distribution instead. Historically this is usually not a very big problem. The requirement that our sample size is at least 30 is a "rule of thumb" and not based on rigorous statistical theory. 

Below are a summary of these assumptions restated: 

- All our observations are independent, and have the same distribution (come from same population)

- Sample size is greater than 30 ($n\geq 30$)

- The mean and the variance of the distribution the data comes from are finite ($|\mu|< \infty, |\sigma^2|< \infty$)

Once we have verified these assumptions, we still need to determine what the mean ($\mu$) and variance ($\sigma^2$) are.  If we know that our distribution is one of the really common ones above, then this task becomes much eaiser. 


### The Data 


We will see how well the CLT theorem applies to a data set that has a uniform distribution.  That is, say we have a sample of 50 observations which we believe are all uniformly distributed between -5 and 5. We want to determine if the mean of a sample of this type would be normally distributed according to the CLT.  To do so, we simulate a sample of this type 10,000 times and assess if it matches the CLT results. 

```{r}
# Simulate a sample of 50 observations that are uniformly distributed 
gen_unif_mean = function(the_min, the_max){
        gen_data = runif(50, the_min, the_max)
        the_mean = mean(gen_data)
        the_parameters = c(the_min, the_max)
        results = list(the_mean, the_parameters)
        names(results) = c("mean", "parameters")
        return(results)
}

# Simulate 10000 samples, each of size 50, where the minumum value is -5
# And the maximum value is 5
set.seed(123)
sim_means <- replicate(10000, gen_unif_mean(the_min = -5, the_max=5)$mean)


# histogram of results
hist(sim_means, freq = FALSE)
```


To add a curve to the graph we need to know the mean and the standard deviation of the data we generated.  For a uniform distribution with a minimum of -5 and a maximum of 5, the mean is 0 and the standard deviation is $10/\sqrt{12} \approx 2.89$.  Thus according to CLT, we expect our histogram above to correspond to a normal density with mean 0 and standard deviation $2.89/\sqrt{50} \approx 0.41$. 


```{r}

# histogram of results
hist(sim_means, freq = FALSE)

# Add normal curve according to CLT
curve(dnorm(x, mean = 0, sd = 0.41), add = TRUE)
```


## Example: Coin Flips 

Suppose we are interested in the proportion of times we observe a head if we flip a fair sided coin.  A fair question to ask: "how many simulations do we need to do this?".  There are rigorous rules that we can use to determine how many simulations we need until our data set starts to represent the truth; however, we will focus on summary tables and visualization tools for now.  We know we have enough simulations when our data set seems to "stabilize".  





Suppose we flip a coin 10 times and we observe the following results.  

- Heads, Heads, Tails, Tails, Tails, Tails, Heads, Tails, Tails, Tails 

With simulation studies we often want to visualize are results using summary tables and graphs.  Lets create a table of the proportion of heads vs the number of trials.

- Create a plot of the proportion of heads vs the number of trials 




<!--chapter:end:15-simulation.Rmd-->


# Tidyverse 

Placeholder


## Piping Operator 
## Tibbles vs Data Frames
## Key Functions 
### General properties 
### select()
### filter()
### arrange()
### rename()
### mutate()
### group_by()
### summarize()/summarise()
## Examples 
### Example 1
### Example 2
### Example 3
## Additional Resources {-}

<!--chapter:end:16-Tidyverse.Rmd-->


# ggplot2

Placeholder


## Tidyverse and ggplot2
## Intro to ggplot2
### What is the grammar of graphics?{-}
### ggplot2 versus other R Graphics {-}
## Getting started: Kep Components
## Adjust aesthetic attributes by variable 
## Adjust aesthetic attributes for the entire plot 
## Faceting 
## Some example plots
## Where can I learn more?

<!--chapter:end:17-ggplot2.Rmd-->


#  Apply Family of Functions {#apply}

Placeholder


## apply() 
### Example: Built In Function {-}
### Example: User Defined Function {-}
### Returned Objects
### Example: Extra Arguments, Array Output
### Example: List Output
## lapply() 
## sapply()
## tapply() 
## mapply() 
## replicate() 
## How to Pick a Method
## More Examples
### Create a New Variable
## Additional Resources {-}

<!--chapter:end:18-apply-family.Rmd-->


# Debugging 

Placeholder


## Debugging Background (History, Types of Bugs, Challenges to Consider) 
### Types of bugs {-}
### Debugging Process Difficulties {-}
## Debugging Strategies In General
### Example of the Binary Search Algorithm
## Using Debuggers
### traceback() 
### Example with traceback(){-}
### Unfamiliar function with traceback() {-}
### browswer()
### debug() 
## More options to consider
## Important Takeaways  
### Tips for Writing good code {-}
### Steps to Reduce the Number of Bugs {-}
### After You Located the Error {-}
## Resources

<!--chapter:end:19-debugging.Rmd-->

